---
phase: 10-backend-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/db/019_bug_dashboard.sql
  - server/services/BugService.js
  - package.json
autonomous: true

must_haves:
  truths:
    - "Database tables bug_uploads, bugs, weekly_kpis exist with proper indexes"
    - "BugService can parse CSV and validate required columns"
    - "BugService can extract component from labels/summary"
    - "BugService can calculate resolution_time_hours"
    - "CASCADE DELETE removes bugs and KPIs when upload deleted"
  artifacts:
    - path: "server/db/019_bug_dashboard.sql"
      provides: "Database migration for bug dashboard tables"
      contains: "CREATE TABLE IF NOT EXISTS bug_uploads"
    - path: "server/services/BugService.js"
      provides: "CSV parsing and validation logic"
      exports: ["parseCSV", "validateColumns", "extractComponent"]
  key_links:
    - from: "server/services/BugService.js"
      to: "server/db/019_bug_dashboard.sql"
      via: "query function to bug_uploads, bugs tables"
      pattern: "INSERT INTO bug_uploads"
---

<objective>
Create the database schema and BugService foundation for the DevOps Bug Dashboard.

Purpose: Establish the data layer that stores bug uploads, parsed bug data, and pre-calculated KPIs. This is the foundation for CSV upload processing and KPI calculations.

Output:
- Migration file 019_bug_dashboard.sql with three tables
- BugService.js with CSV parsing, validation, and data enrichment
- Dependencies installed (fast-csv, multer)
</objective>

<execution_context>
@/Users/i306072/.claude/get-shit-done/workflows/execute-plan.md
@/Users/i306072/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-backend-foundation/10-RESEARCH.md

Reference existing patterns:
@server/db/018_capture_framework.sql
@server/services/JiraService.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create database migration for bug dashboard tables</name>
  <files>server/db/019_bug_dashboard.sql</files>
  <action>
Create migration file with three tables following existing migration patterns:

**bug_uploads table (DB-01):**
```sql
CREATE TABLE IF NOT EXISTS bug_uploads (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id VARCHAR(255) NOT NULL,
  week_ending DATE NOT NULL,
  filename VARCHAR(255) NOT NULL,
  bug_count INTEGER NOT NULL DEFAULT 0,
  uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  UNIQUE(user_id, week_ending)
);
```

**bugs table (DB-02):**
```sql
CREATE TABLE IF NOT EXISTS bugs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id UUID NOT NULL REFERENCES bug_uploads(id) ON DELETE CASCADE,
  bug_key VARCHAR(50) NOT NULL,
  summary TEXT,
  priority VARCHAR(50),
  status VARCHAR(100),
  created_date TIMESTAMP,
  resolved_date TIMESTAMP,
  resolution_time_hours FLOAT,
  reporter VARCHAR(255),
  assignee VARCHAR(255),
  labels TEXT[],
  component VARCHAR(100),
  raw_data JSONB,
  UNIQUE(upload_id, bug_key)
);
```

**weekly_kpis table (DB-03):**
```sql
CREATE TABLE IF NOT EXISTS weekly_kpis (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  upload_id UUID NOT NULL REFERENCES bug_uploads(id) ON DELETE CASCADE,
  component VARCHAR(100),
  -- KPI values stored as JSONB for flexibility
  kpi_data JSONB NOT NULL DEFAULT '{}',
  calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  UNIQUE(upload_id, component)
);
```

**Indexes (DB-04):**
- idx_bug_uploads_user_id ON bug_uploads(user_id)
- idx_bug_uploads_week_ending ON bug_uploads(week_ending)
- idx_bugs_upload_id ON bugs(upload_id)
- idx_bugs_status ON bugs(status)
- idx_bugs_priority ON bugs(priority)
- idx_bugs_component ON bugs(component)
- idx_bugs_created_date ON bugs(created_date)
- idx_weekly_kpis_upload_id ON weekly_kpis(upload_id)

**CASCADE DELETE (DB-05):**
- bugs.upload_id references bug_uploads(id) ON DELETE CASCADE
- weekly_kpis.upload_id references bug_uploads(id) ON DELETE CASCADE

Add update_updated_date_column trigger for bug_uploads table.
  </action>
  <verify>
```bash
# Verify migration file exists and has all tables
grep -c "CREATE TABLE" server/db/019_bug_dashboard.sql
# Expected: 3 tables

# Verify CASCADE DELETE
grep -c "ON DELETE CASCADE" server/db/019_bug_dashboard.sql
# Expected: 2+ occurrences
```
  </verify>
  <done>Migration file has bug_uploads, bugs, weekly_kpis tables with proper indexes and cascade delete</done>
</task>

<task type="auto">
  <name>Task 2: Install dependencies and create BugService foundation</name>
  <files>package.json, server/services/BugService.js</files>
  <action>
**Step 1: Install dependencies**
```bash
npm install fast-csv multer
```

**Step 2: Create BugService.js following JiraService pattern**

Create server/services/BugService.js with:

```javascript
import { query, pool } from '../db/connection.js';
import { parse } from 'fast-csv';
import { Readable } from 'stream';

class BugService {
  // Required CSV columns for validation (UPLOAD-03)
  static REQUIRED_COLUMNS = ['Key', 'Summary', 'Priority', 'Status', 'Created', 'Resolved', 'Reporter', 'Assignee', 'Labels'];

  // Component extraction priority order
  static COMPONENT_PATTERNS = [
    { pattern: 'deploy', component: 'deployment' },
    { pattern: 'foss', component: 'foss-vulnerabilities' },
    { pattern: 'vulnerability', component: 'foss-vulnerabilities' },
    { pattern: 'broker', component: 'service-broker' },
    { pattern: 'cm-metering', component: 'cm-metering' },
    { pattern: 'sdm-metering', component: 'sdm-metering' }
  ];

  /**
   * Parse CSV buffer into bug objects with validation
   * @param {Buffer} fileBuffer - CSV file content
   * @returns {Promise<Array>} - Parsed bug objects
   * @throws {Error} - If required columns missing
   */
  async parseCSV(fileBuffer) {
    return new Promise((resolve, reject) => {
      const bugs = [];
      let headersValidated = false;

      const stream = parse({ headers: true, trim: true })
        .on('data', (row) => {
          // Validate headers on first row
          if (!headersValidated) {
            const missingColumns = BugService.REQUIRED_COLUMNS.filter(col => !(col in row));
            if (missingColumns.length > 0) {
              stream.destroy();
              reject(new Error(`Missing required columns: ${missingColumns.join(', ')}`));
              return;
            }
            headersValidated = true;
          }

          // Map CSV row to bug object
          bugs.push({
            bug_key: row.Key,
            summary: row.Summary,
            priority: row.Priority,
            status: row.Status,
            created_date: this.parseDate(row.Created),
            resolved_date: row.Resolved ? this.parseDate(row.Resolved) : null,
            reporter: row.Reporter,
            assignee: row.Assignee || null,
            labels: row.Labels ? row.Labels.split(',').map(l => l.trim()).filter(Boolean) : [],
            raw_data: row
          });
        })
        .on('error', reject)
        .on('end', () => resolve(bugs));

      // Convert buffer to readable stream
      Readable.from(fileBuffer.toString()).pipe(stream);
    });
  }

  /**
   * Parse date string with multiple format fallbacks
   * JIRA exports vary by locale - try common formats
   */
  parseDate(dateString) {
    if (!dateString || dateString.trim() === '') return null;

    // Try ISO format with space: "2025-01-15 10:30:45"
    let date = new Date(dateString.replace(' ', 'T'));
    if (!isNaN(date.getTime())) return date;

    // Try standard Date constructor
    date = new Date(dateString);
    if (!isNaN(date.getTime())) return date;

    // Try DD/MM/YYYY HH:mm format
    const ddmmyyyy = dateString.match(/(\d{2})\/(\d{2})\/(\d{4})\s+(\d{2}):(\d{2})/);
    if (ddmmyyyy) {
      const [, day, month, year, hour, minute] = ddmmyyyy;
      date = new Date(Date.UTC(parseInt(year), parseInt(month) - 1, parseInt(day), parseInt(hour), parseInt(minute)));
      if (!isNaN(date.getTime())) return date;
    }

    console.warn(`Could not parse date: ${dateString}`);
    return null;
  }

  /**
   * Extract component from labels and summary
   * Priority order: deployment > foss > service-broker > cm-metering > sdm-metering > other
   */
  extractComponent(bug) {
    const labelsStr = (bug.labels || []).join(' ').toLowerCase();
    const summary = (bug.summary || '').toLowerCase();
    const combined = `${labelsStr} ${summary}`;

    for (const { pattern, component } of BugService.COMPONENT_PATTERNS) {
      if (combined.includes(pattern)) {
        return component;
      }
    }
    return 'other';
  }

  /**
   * Calculate resolution time in hours
   */
  calculateResolutionTime(bug) {
    if (!bug.created_date || !bug.resolved_date) return null;
    const created = new Date(bug.created_date);
    const resolved = new Date(bug.resolved_date);
    if (isNaN(created.getTime()) || isNaN(resolved.getTime())) return null;
    return (resolved - created) / (1000 * 60 * 60); // Convert ms to hours
  }

  /**
   * Enrich bugs with calculated fields
   */
  enrichBugs(bugs) {
    return bugs.map(bug => ({
      ...bug,
      component: this.extractComponent(bug),
      resolution_time_hours: this.calculateResolutionTime(bug)
    }));
  }

  // ============================================
  // Upload Operations (called from routes)
  // ============================================

  /**
   * List all uploads for a user
   */
  async listUploads(userId) {
    const sql = `
      SELECT id, week_ending, filename, bug_count, uploaded_at
      FROM bug_uploads
      WHERE user_id = $1
      ORDER BY week_ending DESC
    `;
    const result = await query(sql, [userId]);
    return result.rows;
  }

  /**
   * Get upload by ID (with user_id check for security)
   */
  async getUpload(userId, uploadId) {
    const sql = `
      SELECT * FROM bug_uploads
      WHERE id = $1 AND user_id = $2
    `;
    const result = await query(sql, [uploadId, userId]);
    return result.rows[0] || null;
  }

  /**
   * Check if upload exists for a week
   */
  async getUploadByWeek(userId, weekEnding) {
    const sql = `
      SELECT id, filename, bug_count, uploaded_at
      FROM bug_uploads
      WHERE user_id = $1 AND week_ending = $2
    `;
    const result = await query(sql, [userId, weekEnding]);
    return result.rows[0] || null;
  }

  /**
   * Delete upload (cascade deletes bugs and KPIs via DB constraint)
   */
  async deleteUpload(userId, uploadId) {
    const sql = `
      DELETE FROM bug_uploads
      WHERE id = $1 AND user_id = $2
      RETURNING id
    `;
    const result = await query(sql, [uploadId, userId]);
    return result.rowCount > 0;
  }
}

export default new BugService();
```

Key implementation notes:
- Follow JiraService singleton pattern (export default new BugService())
- REQUIRED_COLUMNS matches JIRA export format
- Component extraction uses priority order from research
- Date parsing handles multiple JIRA export formats
- All methods take userId for multi-tenancy
  </action>
  <verify>
```bash
# Verify dependencies installed
grep -q '"fast-csv"' package.json && echo "fast-csv installed"
grep -q '"multer"' package.json && echo "multer installed"

# Verify BugService exists with key methods
grep -c "async parseCSV" server/services/BugService.js
grep -c "extractComponent" server/services/BugService.js
grep -c "REQUIRED_COLUMNS" server/services/BugService.js
```
  </verify>
  <done>Dependencies installed, BugService.js has parseCSV, extractComponent, calculateResolutionTime, enrichBugs, and CRUD methods</done>
</task>

<task type="auto">
  <name>Task 3: Run migration and verify schema</name>
  <files>server/db/019_bug_dashboard.sql</files>
  <action>
Run the migration to create tables:

```bash
npm run migrate
```

If local PostgreSQL not running, verify migration file is correct by:
1. Checking SQL syntax is valid
2. Verifying all three tables defined
3. Ensuring indexes are created
4. Confirming cascade delete relationships

Test that BugService can be imported without errors:
```bash
node -e "import('./server/services/BugService.js').then(m => console.log('BugService loaded:', Object.keys(m.default).slice(0,5)))"
```
  </action>
  <verify>
```bash
# Run migration (will show version executed)
npm run migrate 2>&1 | head -20

# Verify service loads without errors
node -e "import('./server/services/BugService.js').then(m => console.log('OK'))" 2>&1
```
  </verify>
  <done>Migration executed successfully, BugService imports without errors</done>
</task>

</tasks>

<verification>
Phase 10-01 verification:

1. **Database tables exist** (DB-01, DB-02, DB-03):
   - bug_uploads table with user_id, week_ending, filename, bug_count
   - bugs table with upload_id FK, calculated fields
   - weekly_kpis table with upload_id FK

2. **Indexes created** (DB-04):
   - Indexes on user_id, status, priority, component for query performance

3. **Cascade delete works** (DB-05):
   - Deleting bug_upload cascades to bugs and weekly_kpis

4. **CSV validation** (UPLOAD-03):
   - BugService.REQUIRED_COLUMNS defines expected columns
   - parseCSV throws error with missing column names

5. **Dependencies installed**:
   - fast-csv in package.json
   - multer in package.json
</verification>

<success_criteria>
- [ ] server/db/019_bug_dashboard.sql exists with 3 tables
- [ ] CASCADE DELETE defined on bugs.upload_id and weekly_kpis.upload_id
- [ ] Indexes created for user_id, status, priority, component
- [ ] fast-csv and multer in package.json dependencies
- [ ] BugService.js exports parseCSV, extractComponent methods
- [ ] BugService imports without errors
- [ ] npm run migrate executes successfully
</success_criteria>

<output>
After completion, create `.planning/phases/10-backend-foundation/10-01-SUMMARY.md`
</output>
